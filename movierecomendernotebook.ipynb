{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJbrOXzh2Fw3"
   },
   "source": [
    "### Table of Contents\n",
    "---\n",
    "1. [Introduction](#intro)\n",
    " * Background Infornation\n",
    "\n",
    "---\n",
    "2. [Connecting to Comet](#Comet)\n",
    " * Loading colaborating tool\n",
    " ---\n",
    "3. [Load Dependencies](#imports)\n",
    " * Imports\n",
    " \n",
    "---\n",
    "4. [Exploratory Data Analysis](#EDA)\n",
    " * Data Summary\n",
    " * Missing Values\n",
    " * Ratings\n",
    " * Genres\n",
    " * IMDB Metadata\n",
    "---\n",
    "5. [Data Preprocessing](#preprocessing)\n",
    " * Multidimensional Scaling\n",
    " * Principle Component Analysis\n",
    " * Cluster Analysis\n",
    "---\n",
    "6. [Modelling](#modelling)\n",
    " * Collaborative Filtering\n",
    " * Content-based Filtering\n",
    "---\n",
    "7. [Performance Evaluation](#evaluation)\n",
    " * Root Mean Squared Error (RMSE)\n",
    " * Cross Validation\n",
    "---\n",
    "8. [Model Analysis](#analysis)\n",
    " * Hyperparameter Tuning\n",
    " * Results\n",
    "---\n",
    "9. [Conclusion](#conclusion)\n",
    "---\n",
    "10. [Save Output](#ref)\n",
    "---\n",
    "11. [References](#save)\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuWPzFFu2Fw4"
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "## 1. Introduction  \n",
    "\n",
    "### Background\n",
    "\n",
    "Movie recommender systems are very critical in  ensuring movie lovers can easily get  a glimpse of what type of movie content they engage with daily.\n",
    "\n",
    "\n",
    "\n",
    "Current recommendation systems are content-based filtering and collaborative  filtering\n",
    "\n",
    "#### Content-based filtering\n",
    "\n",
    "This makes recommendations based on user preferences for product features. It is able to recommend new items, but is limited by the need for more data of user preference to improve the quality of recommendations.\n",
    "\n",
    "#### Collaborative filtering\n",
    "\n",
    "Collaborative filtering mimics user-to-user recommendations.\n",
    "\n",
    "#### Hybrid systems\n",
    "\n",
    "A combination of these two recommendations systems is called a hybrid system. They mix the features of the item itself and the preferences of other users\n",
    "\n",
    "### Problem Statement\n",
    "Construct a recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed, based on their historical preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9Q_MjhR2Fw5"
   },
   "source": [
    "<a id=\"imports\"></a>\n",
    "## 2. Connecting to Comet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL3CdVrnvTf9"
   },
   "source": [
    "Loading Comet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD7hyh4YvTgB"
   },
   "source": [
    "### Libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2L4RWtdn3wG"
   },
   "outputs": [],
   "source": [
    "# ! pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "sHYujSMU2Fw7",
    "outputId": "9af02514-f8d9-4b76-93ad-2eb0b5ff2dce"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csv import reader\n",
    "# Data Preprocessing\n",
    "import random\n",
    "from time import time\n",
    "import cufflinks as cf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Models\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import SVD, NormalPredictor, BaselineOnly, NMF, SlopeOne, CoClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Performance Evaluation\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "\n",
    "# Display\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"white\")\n",
    "pd.set_option('display.max_columns', 37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQmv_5BqvTgF"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQJ7ABb7vTgF"
   },
   "source": [
    "#### Train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "NG1CExwQovfk",
    "outputId": "a394a1d9-7d53-4e87-8692-afa807262336"
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPssD08BvTgG"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/movies-recomendation/train.csv')\n",
    "# test_df = pd.read_csv('/movies-recomendation/test.csv')\n",
    "# print(train_df.shape, test_df.shape)\n",
    "# train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_-nDjXIvTgI"
   },
   "source": [
    "It is noted that the train data includes an additiona column \"timestamp\" This data can be safely dropped as there is no reasonable link between the time someone watches a movie and whether or not they rate it favourably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzQw_WAYvTgJ"
   },
   "source": [
    "#### Movie Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeWu9DNnvTgJ",
    "outputId": "12634c7b-5210-4101-fbb4-816ce80591fb"
   },
   "outputs": [],
   "source": [
    "movies_df =  pd.read_csv('movies.csv', index_col='movieId')\n",
    "imdb_df =  pd.read_csv('imdb_data.csv', index_col='movieId')\n",
    "links_df =  pd.read_csv('links.csv', index_col='movieId')\n",
    "genome_scores =  pd.read_csv('genome_scores.csv', index_col='movieId')\n",
    "genome_tags =  pd.read_csv('genome_tags.csv', index_col='tagId')\n",
    "print(movies_df.shape, imdb_df.shape, links_df.shape, genome_scores.shape, genome_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxsNpbySo_qC",
    "outputId": "e7af633e-dc6a-4c18-c591-c095b61dfe32"
   },
   "outputs": [],
   "source": [
    "# tags = pd.read_csv('datasets/tags.csv')\n",
    "movies_df = pd.read_csv('movies.csv')\n",
    "imdb_df = pd.read_csv('imdb_data.csv')\n",
    "links_df = pd.read_csv('links.csv')\n",
    "genome_tags = pd.read_csv('genome_tags.csv')\n",
    "genome_scores = pd.read_csv('genome_scores.csv')\n",
    "# sample_submission = pd.read_csv('datasets/sample_submission.csv')\n",
    "print(movies_df.shape, imdb_df.shape, links_df.shape, genome_scores.shape, genome_tags.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5hj30DdL9eq"
   },
   "source": [
    "<a id=\"EDA\"></a>\n",
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ96l2eRWPvH"
   },
   "source": [
    "### Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IvQ-TzWWPvI",
    "outputId": "edb8651c-3413-41a7-ce2a-f4fa336cc06c"
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilvpG2WfWPvL",
    "outputId": "4f9ff8ef-dc3f-4c07-e918-f809f1745be9"
   },
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqhQUhPQWPvN",
    "outputId": "7589ef21-c682-46a0-b968-1200b3221c3f"
   },
   "outputs": [],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUUxt8m_WPvQ",
    "outputId": "8e54b06a-aa61-4f6c-bbe9-3c0b36fe9a2a"
   },
   "outputs": [],
   "source": [
    "imdb_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8SokdNRWPvS",
    "outputId": "327c22ef-394e-48bc-f4a4-b5bf1bcff722"
   },
   "outputs": [],
   "source": [
    "links_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EprO8B29WPvV",
    "outputId": "3977021e-3452-4c10-810e-90ef227e0b9b"
   },
   "outputs": [],
   "source": [
    "genome_scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJDV42lqWPvX",
    "outputId": "56ee4ac7-3da0-4036-f6c5-a166f2f99231"
   },
   "outputs": [],
   "source": [
    "genome_tags.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-VrLcd_WPvZ"
   },
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVyATHsuWPva",
    "outputId": "df4fea3e-e13e-45c3-d180-35ffd36e5c4a"
   },
   "outputs": [],
   "source": [
    "print(\"Train: \")\n",
    "print(str(train_df.isnull().sum()))\n",
    "print(\"************\")\n",
    "print(\"Test: \")\n",
    "print(str(test_df.isnull().sum()))\n",
    "print(\"************\")\n",
    "print(\"Movies: \")\n",
    "print(str(movies_df.isnull().sum()))\n",
    "print(\"************\")\n",
    "print(\"Links: \")\n",
    "print(str(links_df.isnull().sum()))\n",
    "print(\"************\")\n",
    "print(\"IMDB: \")\n",
    "print(str(imdb_df.isnull().sum()))\n",
    "print(\"************\")\n",
    "print(\"Genome scores: \")\n",
    "print(str(genome_scores.isnull().sum()))\n",
    "print(\"************\")\n",
    "print(\"Genome tags: \")\n",
    "print(str(genome_tags.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDahYSOZWPvc"
   },
   "outputs": [],
   "source": [
    "# Drop missing rows\n",
    "\n",
    "links_df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEh0iZUFvTgN"
   },
   "source": [
    "### Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM_w0_BEvTgO"
   },
   "source": [
    "Q: Which users have rated the most movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4OZz6bPvTgO"
   },
   "outputs": [],
   "source": [
    "def user_ratings_count(df, n):\n",
    "    \"\"\"\n",
    "    Counts the number of user ratings.\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): input DataFrame\n",
    "        n (int): number of users to show\n",
    "    Returns\n",
    "    -------\n",
    "        barplot (NoneType): barplot of top n users by number of observations\n",
    "    Example\n",
    "    -------\n",
    "        >>> df = pd.DataFrame({'userId':[1,2,3,1,2,4,5,4]})\n",
    "        >>> user_ratings_count(df, 3)\n",
    "            NoneType (barplot)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,6))\n",
    "    data = df['userId'].value_counts().head(n)\n",
    "    ax = sns.barplot(x = data.index, y = data, order= data.index, palette='brg', edgecolor=\"black\")\n",
    "    for p in ax.patches:\n",
    "        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n",
    "    plt.title(f'Top {n} Users by Number of Ratings', fontsize=14)\n",
    "    plt.xlabel('User ID')\n",
    "    plt.ylabel('Number of Ratings')\n",
    "    print(\"Combined number of ratings:\\t\",df['userId'].value_counts().head(n).sum(),\n",
    "         \"\\nTotal number of movies:\\t\\t\", df['movieId'].nunique())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "zigw2K7SvTgQ",
    "outputId": "0ed57f15-7aa3-447a-cfbd-e792ea406bb7"
   },
   "outputs": [],
   "source": [
    "user_ratings_count(train_df,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DEdxq0oWPvo"
   },
   "source": [
    "Fig 1. User 72315 has rated an extreme number of movies relative to other users. For EDA purposes, this user is removed to make interpretation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxeoBdvhvTgT"
   },
   "outputs": [],
   "source": [
    "# Exclude user 72315 for EDA\n",
    "eda_df = train_df[train_df['userId']!=72315]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "zjmlDrhqvTgV",
    "outputId": "333dde6d-ef83-4e8c-efcc-7c85eb2c3739"
   },
   "outputs": [],
   "source": [
    "user_ratings_count(eda_df,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a38FeX4WPvt",
    "outputId": "f1b75bf9-18bd-43f4-f282-437131fdebd7"
   },
   "outputs": [],
   "source": [
    "# How many ratings have we lost?\n",
    "34398-23734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uiItZNaLlwtQ",
    "outputId": "84798d02-e03a-489d-ef32-d52ce3ad97af"
   },
   "outputs": [],
   "source": [
    "# How many movies were only rated by our outlier?\n",
    "48213-45844"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nucmj470WPvw"
   },
   "source": [
    "Our outlier user clearly rated quite a few movies. The loss of unique ratings indicates that he could've been the only person watching those movies. This would cause the recommender to use him as a baseline for those movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjV-QVRnWPvw"
   },
   "source": [
    "Q: How do users tend to rate movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwkMSSavvTgY"
   },
   "outputs": [],
   "source": [
    "def ratings_distplot(df, column='rating'):\n",
    "    \"\"\"\n",
    "    Plots the distribution of ratings in the dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): input DataFrame\n",
    "        column (str): column to plot\n",
    "    Returns\n",
    "    -------\n",
    "        distplot (NoneType): distplot of rating frequencies\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,6))\n",
    "    ax = sns.distplot(df[f'{column}'],bins=10, kde=False, hist_kws=dict(alpha=0.6),color=\"#4D17A0\")\n",
    "    mean = df[f'{column}'].mean()\n",
    "    median = df[f'{column}'].median()\n",
    "    plt.axvline(x=mean, label = f'mean {round(mean,2)}' , color='#4D17A0', lw=3, ls = '--')\n",
    "    plt.axvline(x=median, label = f'median {median}' , color='#4DA017', lw=3, ls = '--')\n",
    "    plt.xlim((0.5,5))\n",
    "    plt.ylim((0,2500000))\n",
    "    plt.title(f'Distribution of Ratings', fontsize=14)\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "kqgMW2z6vTga",
    "outputId": "78b88c70-7e55-49ad-be8f-6bd0a4792322"
   },
   "outputs": [],
   "source": [
    "ratings_distplot(eda_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoeyfSkiWPv2"
   },
   "source": [
    "It is interesting that the ratings are left-skewed. It was expected that there would be a normal distrubtion with a mean rating of 3. Instead, we observe that users tend to rate movies quite favourably and tend to avoid negative ratings. This skew might be explained by the tendency of users to rate movies they liked. In other words, if a user doesn't like a movie, it is unlikely that they will watch it through to the end, let alone rate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OouqN1vWPv2"
   },
   "source": [
    "Q: Is there a relationship between the number of movies a user has rated and the rating that they give?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JECfZm57vTgd"
   },
   "outputs": [],
   "source": [
    "def mean_ratings_scatter(df, color='#4DA017', column='userId'):\n",
    "    \"\"\"\n",
    "    Make scatterplots of mean ratings.\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): input DataFrame\n",
    "        color (str): plot colour\n",
    "        column (str): column to plot\n",
    "    Returns\n",
    "    -------\n",
    "        scatterplot (NoneType): scatterplot of mean number of ratings\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6,4))\n",
    "    mean_ratings = df.groupby(f'{column}')['rating'].mean()\n",
    "    user_counts = df.groupby(f'{column}')['movieId'].count().values\n",
    "    sns.scatterplot(x=mean_ratings, y = user_counts, color=color)\n",
    "    plt.title(f'Mean Ratings by Number of Ratings', fontsize=14)\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Number of Ratings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "2NQRq3suvTgf",
    "outputId": "b910ecbb-f94b-46ee-d892-7151f6b14959"
   },
   "outputs": [],
   "source": [
    "# Mean user ratings by number of ratings\n",
    "mean_ratings_scatter(eda_df,'#4D17A0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHfZgs4eWPv7"
   },
   "source": [
    "There doesn't seem to be a relationship, as the number of ratings and how a user rates a movie do not show any correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GbadhNZWPv8"
   },
   "source": [
    "Q: Is there a relationship between the number of ratings a movie has and how highly it is rated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "VSh57c0vvTgi",
    "outputId": "8c0bdee4-f1b9-4eb6-991f-5ba2535aa46b"
   },
   "outputs": [],
   "source": [
    "# Mean movie ratings by number of ratings\n",
    "mean_ratings_scatter(eda_df, column='movieId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TU5rRvD-WPv-"
   },
   "source": [
    "This time we do see a relationship, The more ratings a movie has, the more highly it is likely to be rated. This confirms our intuitive understanding that the more highly rated a movie is, the more likely is that viewers will recommend the movie to each other. In other words, people generally try to avoid maing bad recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhTMhPRrWPv-"
   },
   "source": [
    "Q: Which are the best and worst rated movies of all time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwBdLb-TvTgl"
   },
   "outputs": [],
   "source": [
    "def plot_ratings(count, n, color='#4DA017', best=True, method='mean'):\n",
    "    \"\"\"\n",
    "    Make scatterplots of ratings.\n",
    "    Parameters\n",
    "    ----------\n",
    "        count (int): number of ratings threshold\n",
    "        n (int): number of movies\n",
    "        color (str): plot colour\n",
    "        best (bool): column to plot\n",
    "        method (str): statistical measure\n",
    "    Returns\n",
    "    -------\n",
    "        scatterplot (NoneType): scatterplot of mean number of ratings\n",
    "    \"\"\"\n",
    "    # What are the best and worst movies\n",
    "    # Creating a new DF with mean and count\n",
    "    if method == 'mean':\n",
    "        movie_avg_ratings = pd.DataFrame(eda_df.join(movies_df, on='movieId', how='left').groupby(['movieId', 'title'])['rating'].mean())\n",
    "    else:\n",
    "        movie_avg_ratings = pd.DataFrame(eda_df.join(movies_df, on='movieId', how='left').groupby(['movieId', 'title'])['rating'].median())\n",
    "    movie_avg_ratings['count'] = eda_df.groupby('movieId')['userId'].count().values\n",
    "    movie_avg_ratings.reset_index(inplace=True)\n",
    "    movie_avg_ratings.set_index('movieId', inplace=True)\n",
    "\n",
    "    # Remove movies that have been rated fewer than n times\n",
    "    data = movie_avg_ratings[movie_avg_ratings['count']>count]\n",
    "    data.sort_values('rating', inplace= True,ascending=False)\n",
    "    if best == True:\n",
    "        plot = data.head(n).sort_values('rating', ascending=True)\n",
    "        title='Best Rated'\n",
    "    else:\n",
    "        plot = data.tail(n).sort_values('rating', ascending=False)\n",
    "        title='Worst Rated'\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.scatterplot(x=plot['rating'], y=plot['title'], size=plot['count'], color=color)\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('')\n",
    "    plt.tick_params(axis='y', which='both', labelleft=False, labelright=True)\n",
    "    plt.title(f'Top {n} {title} Movies with Over {count} Ratings', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "7kVZInoNvTgn",
    "outputId": "8a5115c1-ee7c-4165-f2c0-2ca2202c47d3"
   },
   "outputs": [],
   "source": [
    "# What are the top 10 highest rated titles?\n",
    "plot_ratings(10000, 15, '#4D17A0', True, 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BvNeDrlWPwD"
   },
   "source": [
    "By filtering movies with less than 10000 ratings, we find that the most popular movies are unsurprising titles. The Shawshank Redemption and The Godfather unsurprisingly top the list. What is interesting is that Movies made post 2000 don't feature often. Do users have a preference to Older movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuVYPaKQvTgq"
   },
   "outputs": [],
   "source": [
    "# What are the 10 worst rated titles?\n",
    "plot_ratings(500, 15,'#4DA017', False, 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZGuzzY0WPwG"
   },
   "source": [
    "Obviously, people did not like Battlefield too much and with 1200 ratings, they really wanted it to be known. It is interesting how many sequels appear in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-UHfEKCvThH"
   },
   "outputs": [],
   "source": [
    "movieRatingDistGroup = train_df['rating'].value_counts().sort_index().reset_index()\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.barplot(data=movieRatingDistGroup, x='index', y='rating', palette=\"brg\", edgecolor=\"black\", ax=ax)\n",
    "ax.set_xlabel(\"Rating\")\n",
    "ax.set_ylabel('Number of Users')\n",
    "ax.set_yticklabels(['{:,}'.format(int(x)) for x in ax.get_yticks().tolist()])\n",
    "total = float(movieRatingDistGroup['rating'].sum())\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2., height+350, '{0:.2%}'.format(height/total), fontsize=11, ha=\"center\", va='bottom')\n",
    "plt.title('Number of Users Per Rating', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuNlOSP0vThJ"
   },
   "source": [
    "Percentage of users per rating, most movies get a rating of 4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GijCG9PvTgs"
   },
   "source": [
    "### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYpz_KA_vTgt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function that hasn't found a use yet - it was usefull in the app!!\n",
    "def feat_extractor(df, col):\n",
    "    \"\"\"\n",
    "    Returns a list of all unique features in a DataFrame columns separated by \"|\"\n",
    "    \"\"\"\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    feat_set = set()\n",
    "    for i in range(len(df[f'{col}'])):\n",
    "        for feat in df[f'{col}'].iloc[i].split('|'):\n",
    "            feat_set.add(feat)\n",
    "    return sorted([feat for feat in feat_set if feat != \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpAr_LSnvTgv"
   },
   "outputs": [],
   "source": [
    "genres = feat_extractor(movies_df, 'genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcQ4ssdDvTgw"
   },
   "source": [
    "Q: Which genres are the most commonly observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifxlccFevTgx"
   },
   "outputs": [],
   "source": [
    "def feature_frequency(df, column):\n",
    "    \"\"\"\n",
    "    Function to count the number of occurences of metadata such as genre\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): input DataFrame containing movie metadata\n",
    "        column (str): target column to extract features from\n",
    "    Returns\n",
    "    -------\n",
    "        \n",
    "    \"\"\"\n",
    "    # Creat a dict to store values\n",
    "    df = df.dropna(axis=0)\n",
    "    genre_dict = {f'{column}': list(),\n",
    "                 'count': list(),}\n",
    "    # Retrieve a list of all possible genres\n",
    "    print('retrieving features...')\n",
    "    for movie in range(len(df)):\n",
    "        gens = df[f'{column}'].iloc[movie].split('|')\n",
    "        for gen in gens:\n",
    "            if gen not in genre_dict[f'{column}']:\n",
    "                genre_dict[f'{column}'].append(gen)\n",
    "    # count the number of occurences of each genre\n",
    "    print('counting...')\n",
    "    for genre in genre_dict[f'{column}']:\n",
    "        count = 0\n",
    "        for movie in range(len(df)):\n",
    "            gens = df[f'{column}'].iloc[movie].split('|')\n",
    "            if genre in gens:\n",
    "                count += 1\n",
    "        genre_dict['count'].append(count)\n",
    "        \n",
    "        # Calculate metrics\n",
    "    data = pd.DataFrame(genre_dict)\n",
    "    print('done!')\n",
    "    return data\n",
    "genres = feature_frequency(movies_df, 'genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8Oa-HC9vTgy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def feature_count(df, column):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    ax = sns.barplot(y = df[f'{column}'], x = df['count'], palette='brg', orient='h')\n",
    "    plt.title(f'Number of Movies Per {column}', fontsize=14)\n",
    "    plt.ylabel(f'{column}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBkfTOY7vTg0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_count(genres.sort_values(by = 'count', ascending=False), 'genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b2_7g-TvTg2"
   },
   "source": [
    "Drama is the most frequently occuring genre in the database. Approximately 5000 movies have missing genres. We can use the IMDB and TMDB ID's together with the APIs to fill missing data. Further, IMAX is not a genre but rather a proprietary system for mass-viewings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vAi6NzpvTg3"
   },
   "source": [
    "The above figure does not tell us anything about the popularity of the genres, lets calculate a mean rating and append it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QI89snFbvTg8"
   },
   "outputs": [],
   "source": [
    "def mean_calc(feat_df, ratings = eda_df, movies = movies_df, metadata = imdb_df, column = 'genres'):\n",
    "    mean_ratings = pd.DataFrame(ratings.join(movies, on='movieId', how='left').groupby(['movieId'])['rating'].mean())\n",
    "    movie_eda = movies.copy()\n",
    "    movie_eda = movie_eda.join(mean_ratings, on = 'movieId', how = 'left')\n",
    "\n",
    "    # Exclude missing values\n",
    "    movie_eda = movie_eda\n",
    "    movie_eda2 = movie_eda[movie_eda['rating'].notnull()]\n",
    "\n",
    "    means = []\n",
    "    for feat in feat_df[f'{column}']:\n",
    "        mean = round(movie_eda2[movie_eda2[f'{column}'].str.contains(feat)]['rating'].mean(),2)\n",
    "        means.append(mean)\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ9Zxc4SWPwc"
   },
   "outputs": [],
   "source": [
    "genres['mean_rating'] = mean_calc(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5foc5zNWvThA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genres.sort_values('mean_rating', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNLilWnrvThC"
   },
   "source": [
    "Film-Noir describes Hollywood crime dramas, particularly those that emphasize cynical attitudes and sexual motivations. The 1940s and 1950s are generally regarded as the \"classic period\" of American film-noir. These movies have the highest ratings but this may be as a result of it's niche audence. The same logic can be applied to IMAX movies, as such, we will only include genres with a count of 500 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0vgmb3FvThC"
   },
   "outputs": [],
   "source": [
    "def genre_popularity(df):\n",
    "    \"\"\"\n",
    "    Plots the mean rating per genre.\n",
    "    \"\"\"\n",
    "    count_filt = 500\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plot_data = df[df['count']>count_filt]\n",
    "    mean = plot_data['mean_rating'].mean()\n",
    "    min_ = plot_data['mean_rating'].min()\n",
    "    max_ = plot_data['mean_rating'].max()\n",
    "    sns.barplot(y = plot_data['genres'], x = plot_data['mean_rating'], order = plot_data['genres'], orient='h',palette='brg')\n",
    "    plt.axvline(x=mean, label = f'mean {round(mean,1)}' , color='black', lw=1, ls ='--')\n",
    "    plt.axvline(x=min_, label = f'min {round(min_,1)}' , color='#4D17A0', lw=1, ls = '--')\n",
    "    plt.axvline(x=max_, label = f'max {max_}' , color='#4DA017', lw=1,ls = '--')\n",
    "    plt.title(f'Mean Rating Per Genre', fontsize=14)\n",
    "    plt.ylabel('Genre')\n",
    "    plt.xlabel('Mean Rating')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pFvs_5ovThE",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genre_popularity(genres.sort_values('mean_rating', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK5F6NKxvThG"
   },
   "source": [
    "The scores are almost evenly distributed with the exceptions of Documentaries, War, Drama, Musicals, and Romance and Thriller, Action, Sci-Fi, and Horror, which rate higher than average and far below average respectively.  \n",
    "\n",
    "We can also visualuse the distribution of ratings per genre with a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv_CDi1SlwvF"
   },
   "outputs": [],
   "source": [
    "box = mean_ratings.join(movies_df)\n",
    "box['genres'] = box['genres'].str.replace('|', ',')\n",
    "box['genres'] = box['genres'].apply(lambda x: x.split(\",\"))\n",
    "box = box.explode('genres')\n",
    "box = box[box['genres']!='(no genres listed)']\n",
    "\n",
    "fig,axis = plt.subplots(figsize=(10, 8))\n",
    "sns.boxplot(y=box['genres'], x=box['rating'], palette=\"brg\", orient='h', showfliers = False)\n",
    "plt.title(\"Distribution of Ratings Per Genre\", fontsize=14)\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VBsmiPIWPwo"
   },
   "source": [
    "### IMDB Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HynsWlaKWPwo"
   },
   "source": [
    "#### Directors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjiTfzenWPwp"
   },
   "source": [
    "Q: Who are the most common directors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxTcxzS7WPwp"
   },
   "outputs": [],
   "source": [
    "def count_directors(df, count = 10):\n",
    "    \"\"\"\n",
    "    Function to count the most common dircetors in a DataFrame:\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): input dataframe containing imdb metadata\n",
    "        count (int): filter directors with fewer than count films\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        directors (DataFrame): output DataFrame\n",
    "    Examples\n",
    "    --------\n",
    "        >>> df = pd.DataFrame({'imdbid':[0,1,2,3,4,5], 'director': [A,B,A,C,B]})\n",
    "        >>> count_directors(df, count = 1)\n",
    "            |index|director|count|\n",
    "            |0|A|2|\n",
    "            |1|B|2|\n",
    "            |2|C|1|\n",
    "    \"\"\"\n",
    "    directors = pd.DataFrame(df['director'].value_counts()).reset_index()\n",
    "    directors.columns = ['director', 'count']\n",
    "    # Lets only take directors who have made 10 or more movies otherwise we will have to analyze 11000 directors\n",
    "    directors = directors[directors['count']>=count]\n",
    "    return directors.sort_values('count', ascending = False)\n",
    "directors = count_directors(imdb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGElsSiUWPws"
   },
   "source": [
    "Some movies do not have a director listed. once again, the IMDB API can be used to retrieve this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2z8oFQ3wWPws",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_count(directors.head(10), 'director')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck2E8hehWPwt"
   },
   "source": [
    "Once again we need to calculate a mean rating for each director in order to determine who is the most popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZHvjhiKWPwu"
   },
   "outputs": [],
   "source": [
    "def dir_mean(df):\n",
    "    df.set_index('director', inplace=True)\n",
    "\n",
    "    direct_ratings = []\n",
    "    directors_eda = eda_df.join(imdb_df, on = 'movieId', how = 'left')\n",
    "    for director in df.index:\n",
    "        rating = round(directors_eda[directors_eda['director']==director]['rating'].mean(),2)\n",
    "        direct_ratings.append(rating)\n",
    "    df['mean_rating'] = direct_ratings\n",
    "    return df.sort_values('mean_rating', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVmCQ5HZWPwx"
   },
   "outputs": [],
   "source": [
    "directors = dir_mean(directors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVXpUDS0WPw0"
   },
   "outputs": [],
   "source": [
    "directors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp56V5erWPw1"
   },
   "outputs": [],
   "source": [
    "def feat_popularity(df, title = 'feat'):\n",
    "    \"\"\"\n",
    "    Plots the mean rating per director.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plot_data = df.copy()\n",
    "    mean = plot_data['mean_rating'].mean()\n",
    "    min_ = plot_data['mean_rating'].min()\n",
    "    max_ = round(plot_data['mean_rating'].max(),2)\n",
    "    sns.barplot(y = plot_data.index, x = plot_data['mean_rating'], order = plot_data.index, orient='h',palette='brg')\n",
    "    plt.axvline(x=mean, label = f'mean {round(mean,1)}' , color='black', lw=1, ls ='--')\n",
    "    plt.axvline(x=min_, label = f'min {round(min_,1)}' , color='#4D17A0', lw=1, ls = '--')\n",
    "    plt.axvline(x=max_, label = f'max {max_}' , color='#4DA017', lw=1,ls = '--')\n",
    "    plt.title(f'Mean Rating Per {title}', fontsize=14)\n",
    "    plt.ylabel(f'{title}')\n",
    "    plt.xlabel('Mean Rating')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mPAlOFXWPw4"
   },
   "outputs": [],
   "source": [
    "feat_popularity(directors.head(10), 'Director')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbOIT1bSWPw9"
   },
   "source": [
    "Immediately, we see some very well known names, Stephen King and Quentin Tarantino are unsurprisingly top of the list. It begs the question, who are the worst rated directors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMhTV8u4WPw-"
   },
   "outputs": [],
   "source": [
    "feat_popularity(directors.tail(10), 'Director')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVMJrG_eWPxB"
   },
   "source": [
    "It is unfortunate to find Tyler Perry and Akira Toriyama so poorly rated. Tyler Perry is best known for his Madea series of movies. As we saw from the least popular movies, sequels do not perform well and Madea has numerous sequels.\n",
    "\n",
    "Akira Toriyama is the Manga artist behind the Dragon Ball franchise. Dragonball is important to Anime communities because it popularized anime in the west. However, despite its loyal fan base, it remains far from being the best anime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6jONYzo2Fx2"
   },
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvwoNAvBWPxC"
   },
   "source": [
    "### Multidimensional Scaling\n",
    "Multidimensional scaling (MDS) is a technique for visualizing distances between objects on a map, where the distance is known between pairs of the objects.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFet0obSWPxC"
   },
   "outputs": [],
   "source": [
    "# Subset the data to cut down computation time for now\n",
    "genome_score = genome_scores[:10000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j53CcOyFWPxF"
   },
   "outputs": [],
   "source": [
    "# Although scores are in the range of 0-1, there is no harm in scaling\n",
    "scaler_mds = StandardScaler()\n",
    "mds_genome = scaler_mds.fit_transform(genome_score.sample(frac=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CHVEG6UWPxH"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(3, n_jobs = -1, verbose = 2, perplexity = 10, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fpx7usfGWPxL",
    "outputId": "501e718a-7336-4d0a-f9b9-7c61674b10a7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsne.fit(mds_genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "n6hh2GoxWPxN",
    "outputId": "488bdbe3-59a5-43c3-bb12-41e292f8910f"
   },
   "outputs": [],
   "source": [
    "Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Add 3D scatter plot\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(tsne.embedding_[:,0], tsne.embedding_[:,1], tsne.embedding_[:,2], color='#4D17A0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "9400drfVWPxP",
    "outputId": "6fa75575-8049-4a94-adb4-e869b7ff7790",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x = tsne.embedding_[:,0], y = tsne.embedding_[:,1], size=tsne.embedding_[:,2],color='#4DA017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT2hM7v6vThL"
   },
   "source": [
    "### Principal Component Analysis\n",
    "Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. [[4]](#ref4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SALNYG0BvThM"
   },
   "outputs": [],
   "source": [
    "# Manually pivot table as data is too large for in-built functions\n",
    "def pivot_(df):\n",
    "    \"\"\"\n",
    "    Pivots table.\n",
    "    \"\"\"\n",
    "    new_dict = {'movieId':sorted(set(df.index))}\n",
    "    pivoted = pd.DataFrame(new_dict)\n",
    "    tagids = sorted(set(df['tagId']))\n",
    "    for Id in range(len(tagids)):\n",
    "        pivoted[f'{Id+1}'] = list(df[df['tagId'] == Id+1]['relevance'])\n",
    "    return pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "1oUb0R95vThN",
    "outputId": "53ba0b27-dc5f-4a75-8f81-1c70ea3798a5"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted = pivot_(genome_scores).set_index('movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvI19CwqvThQ"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXeQYBiPvThS"
   },
   "source": [
    "Lets make this more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "B6P-g2WGvThS",
    "outputId": "71093f3f-7c97-4080-87b8-fc510e84fd46"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.columns = list(genome_tags['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "FaKE5iqlvThU",
    "outputId": "48c5fe9d-df2f-47ef-dc39-4976c921961b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GX12_4-tvThW"
   },
   "outputs": [],
   "source": [
    "features = [col for col in pca_data_pivoted.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZIjwVzkvThY"
   },
   "source": [
    "#### Scales of measurement\n",
    "It is important that we scale the data before dimensionality reduction.\n",
    "\n",
    "Although all variables are measured on the same scale (0-1), there shouldn't be any downside to setting the mean to zero and standard deviation to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ9xevnQvThZ"
   },
   "outputs": [],
   "source": [
    "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
    "columns = random.sample(range(0, 1129), 20)\n",
    "pca_data_pivoted.iloc[:,columns].iplot(kind='box', title=\"Boxplots of Features (Unscaled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8L9aV1PvThi"
   },
   "outputs": [],
   "source": [
    "def scaler(df):\n",
    "    \"\"\"\n",
    "    Scales data.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler(with_std=True)\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaNuJlshvThl"
   },
   "outputs": [],
   "source": [
    "pca_scaled = scaler(pca_data_pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWprBTLvvThp"
   },
   "outputs": [],
   "source": [
    "scaled_pca = pd.DataFrame(pca_scaled, index = pca_data_pivoted.index, columns = pca_data_pivoted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxL2umW0vThr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
    "# using plotly to plot the boxplot\n",
    "scaled_pca.iloc[:,columns].iplot(kind='box', title=\"Boxplots of Features (Scaled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQAGbWDmvThy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# fit the PCA model to our data and apply the dimensionality reduction \n",
    "prin_comp = pca.fit_transform(pca_data_pivoted[features])\n",
    "\n",
    "# create a dataframe containing the principal components\n",
    "pca_df = pd.DataFrame(data = prin_comp,\n",
    "                      index=pca_data_pivoted.index, columns=pca_data_pivoted.columns\n",
    "                     )\n",
    "\n",
    "# plot line graph of cumulative variance explained\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_),color='#4D17A0')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6HbWiYJvTh0"
   },
   "outputs": [],
   "source": [
    "pca_75 = PCA(.80)\n",
    "pca_75_df = pca_75.fit_transform(pca_data_pivoted)\n",
    "print(round(pca_75.explained_variance_ratio_.sum()*100, 1),\n",
    "      \"% of variance explained by\",\n",
    "      pca_75.n_components_,\n",
    "      \"components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZQihRAblwwO"
   },
   "source": [
    "The cummulative explained variance shows an initially steep then gradual curve and not the sharp elbow we were expecting. This could be a result of the genomes already having been chosen as the principle components of movies. However we can see that 80% of the variance in the movie dataset is explained by 131 components. We should use only these components for computatonal efficiency in a content based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seD8qOOavTh7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_75_df = pd.DataFrame(pca_75_df, index = pca_data_pivoted.index)\n",
    "pca_75_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UYQuJkwvTh7"
   },
   "source": [
    "### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvQgXservTiA"
   },
   "outputs": [],
   "source": [
    "# Manually implement the WCSS\n",
    "def within_cluster_variation(df, label_col='cluster_label'):\n",
    "    \"\"\"\n",
    "    Manually implements the WCSS.\n",
    "    \"\"\"\n",
    "    centroids = df.groupby(label_col).mean()\n",
    "    out = 0\n",
    "    for label, point in centroids.iterrows():\n",
    "        df_features = df[df[label_col] == label].drop(label_col, axis=1)\n",
    "        out += (df_features - point).pow(2).sum(axis=1).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXK98vYcvTiD"
   },
   "outputs": [],
   "source": [
    "# let's try everything between 2 and 18 clusters where 18 is the number of genres\n",
    "n_clusters = np.arange(2, 19)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 2 and 19\n",
    "for k in n_clusters:\n",
    "    print(f'training model with {k} clusters')\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(pca_75_df)\n",
    "    \n",
    "    # measure BCSS\n",
    "    print(f'evaluating model with {k} clusters')\n",
    "    y_preds = km.predict(pca_75_df)\n",
    "    pca_75_df = pd.DataFrame(pca_75_df)\n",
    "    pca_75_df['cluster_label'] = y_preds\n",
    "    errors.append(within_cluster_variation(pca_75_df, 'cluster_label'))\n",
    "    print(errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nbijk0KBvTiG"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method for Determining Optimal Value of k')\n",
    "plt.scatter(n_clusters, errors, color=\"#4DA017\")\n",
    "plt.plot(n_clusters, errors)\n",
    "plt.xticks(n_clusters)\n",
    "plt.axvline(x=3, color='#4D17A0', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiulgkQqvTiI"
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "# Remember to set the random state for reproducibility\n",
    "km = KMeans(n_clusters=K, verbose=0, random_state=42)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(pca_75_df)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvbTC_dDvTiM"
   },
   "outputs": [],
   "source": [
    "# Obtain cluster memberships for each item in the data\n",
    "y_preds = km.predict(pca_75_df)\n",
    "pca_75_df['cluster_label'] = y_preds\n",
    "centers = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS24QugKvTiP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "for k in range(K):\n",
    "    x1 = pca_75_df[pca_75_df['cluster_label'] == k][0]\n",
    "    x2 = pca_75_df[pca_75_df['cluster_label'] == k][1]\n",
    "    plt.scatter(x1, x2, label=\"k = \"+str(k+1),alpha=0.75)\n",
    "# Show cluster centroid locations    \n",
    "plt.scatter(centers[:,0],centers[:,1],label=\"centroid\")\n",
    "plt.legend()\n",
    "plt.title(f\"K = {K}\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chM0AHBJ2FyD"
   },
   "source": [
    "<a id=\"modelling\"></a>\n",
    "## 5. Modelling\n",
    "\n",
    "To reduce computation time, we train and evaluate the following models on a 100k subset of the data. The best performing model will be trained on the whole dataset to predict the ratings for the final submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whaHVhWzWPyD"
   },
   "outputs": [],
   "source": [
    "# Load the 100k dataset\n",
    "train_df.drop('timestamp', axis=1, inplace=True)\n",
    "train_subset = train_df[:100000]\n",
    "reader = Reader(rating_scale=(train_subset['rating'].min(), train_subset['rating'].max()))\n",
    "data = Dataset.load_from_df(train_subset[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbe96XXoWPyF"
   },
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users. It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user [[5]](#ref5).\n",
    "\n",
    "#### SVD\n",
    "\n",
    "The Singular Value Decomposition algorithm is a matrix factorization technique which reduces the number of features of a dataset and was popularized by Simon Funk during the [Neflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) contest [[6]](#ref6). In the matrix structure, each row represents a user and each column represents a movie. The matrix elements are ratings that are given to movies by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFKprc2IWPyG",
    "outputId": "7236c629-c719-40d8-c290-8c1a0c219a95"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 30, n_factors = 200, init_std_dev = 0.05, random_state=42)\n",
    "svd_test.fit(trainset)\n",
    "predictions = svd_test.test(testset)\n",
    "# Calculate RMSE\n",
    "svd_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_x-FhLNWPyI"
   },
   "source": [
    "#### NormalPredictor  \n",
    "The Normal Predictor algorithm predicts a random rating for each movie based on the distribution of the training set, which is assumed to be normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pG2PARN5WPyI",
    "outputId": "c56db62b-2a3f-4f18-abf2-66f4796a1eec"
   },
   "outputs": [],
   "source": [
    "np_test = NormalPredictor()\n",
    "np_test.fit(trainset)\n",
    "predictions = np_test.test(testset)\n",
    "# Calculate RMSE\n",
    "np_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONPKskgFWPyK"
   },
   "source": [
    "#### BaselineOnly  \n",
    "The Baseline Only algorithm predicts the baseline estimate for a given user and movie. A baseline is calculated using either Stochastic Gradient Descent (SGD) or Alternating Least Squares (ALS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B__DCfvfWPyK",
    "outputId": "959b36b0-7d72-4c4e-f179-0f27692c7a87"
   },
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd','n_epochs': 40}\n",
    "blo_test = BaselineOnly(bsl_options=bsl_options)\n",
    "blo_test.fit(trainset)\n",
    "predictions = blo_test.test(testset)\n",
    "# Calculate RMSE\n",
    "blo_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0hdUFg0WPyL"
   },
   "source": [
    "#### NMF  \n",
    "NMF is a collaborative filtering algorithm based on Non-negative Matrix Factorization. The optimization procedure is a (regularized) stochastic gradient descent with a specific choice of step size that ensures non-negativity of factors, provided that their initial values are also positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21IiVZyOWPyM",
    "outputId": "f4e39805-4ffe-4906-b104-bcc32483e360"
   },
   "outputs": [],
   "source": [
    "nmf_test = NMF()\n",
    "nmf_test.fit(trainset)\n",
    "predictions = nmf_test.test(testset)\n",
    "# Calculate RMSE\n",
    "nmf_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rKRlHmDWPyN"
   },
   "source": [
    "#### SlopeOne  \n",
    "The SlopeOne algorithm is a simple yet accurate collaborative filtering algorithm that uses a simple linear regression model to solve the data sparisity problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_meia6EWPyN",
    "outputId": "139eefb7-3d66-4476-9b33-7a7e35259705"
   },
   "outputs": [],
   "source": [
    "slo_test = SlopeOne()\n",
    "slo_test.fit(trainset)\n",
    "predictions = slo_test.test(testset)\n",
    "# Calculate RMSE\n",
    "slo_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvlEikIiWPyO"
   },
   "source": [
    "#### CoClustering  \n",
    "The Co-clustering algorithm assigns clusters using a straightforward optimization method, much like k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-AmTtK0NWPyP",
    "outputId": "613b676b-311c-4fc8-ad93-94f267f04d8e"
   },
   "outputs": [],
   "source": [
    "cc_test = CoClustering(random_state=42)\n",
    "cc_test.fit(trainset)\n",
    "predictions = cc_test.test(testset)\n",
    "# Calculate RMSE\n",
    "cc_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaamIZ82WPyQ"
   },
   "source": [
    "### Content-based Filtering  \n",
    "Content-based filtering, also referred to as cognitive filtering, recommends items based on a comparison between the content of the items and a user profile. The content of each item is represented as a set of descriptors or terms, typically the words that occur in a document [[7]](#ref7). In the following section, the model uses genres as keywords to recommend similar movies based on input from a user. The model was not used to predict ratings for the testing data, as it is too computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuoTVsX3WPyR"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(subset_size):\n",
    "    \"\"\"Prepare data for use within Content filtering algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subset_size : int\n",
    "        Number of movies to use within the algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas Dataframe\n",
    "        Subset of movies selected for content-based filtering.\n",
    "\n",
    "    \"\"\"\n",
    "    # Split genre data into individual words.\n",
    "    movies['keyWords'] = movies['genres'].str.replace('|', ' ')\n",
    "    # Subset of the data\n",
    "    movies_subset = movies[:subset_size]\n",
    "    return movies_subset\n",
    " \n",
    "def content_model(movie_list,top_n=10): \n",
    "    \"\"\"Performs Content filtering based upon a list of movies supplied\n",
    "       by the app user.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    movie_list : list (str)\n",
    "        Favorite movies chosen by the app user.\n",
    "    top_n : type\n",
    "        Number of top recommendations to return to the user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list (str)\n",
    "        Titles of the top-n movie recommendations to the user.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initializing the empty list of recommended movies\n",
    "    data = data_preprocessing(2000)\n",
    "    # Instantiating and generating the count matrix\n",
    "    count_vec = CountVectorizer()\n",
    "    count_matrix = count_vec.fit_transform(data['keyWords'])\n",
    "    indices = pd.Series(data['title'])\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "    cosine_sim = pd.DataFrame(cosine_sim, index = data.index, columns = data.index)\n",
    "    # Getting the index of the movie that matches the title\n",
    "    idx_1 = indices[indices == movie_list[0]].index[0]\n",
    "    idx_2 = indices[indices == movie_list[1]].index[0]\n",
    "    idx_3 = indices[indices == movie_list[2]].index[0]\n",
    "    # Creating a Series with the similarity scores in descending order\n",
    "    rank_1 = cosine_sim[idx_1]\n",
    "    rank_2 = cosine_sim[idx_2]\n",
    "    rank_3 = cosine_sim[idx_3]\n",
    "    # Calculating the scores\n",
    "    score_series_1 = pd.Series(rank_1).sort_values(ascending = False)\n",
    "    score_series_2 = pd.Series(rank_2).sort_values(ascending = False)\n",
    "    score_series_3 = pd.Series(rank_3).sort_values(ascending = False)\n",
    "    # Getting the indexes of the 10 most similar movies\n",
    "    listings = score_series_1.append(score_series_2).append(score_series_3).sort_values(ascending = False)\n",
    "    # Store movie names\n",
    "    recommended_movies = []\n",
    "    # Appending the names of movies\n",
    "    top_50_indexes = list(listings.iloc[1:50].index)\n",
    "    # Removing chosen movies\n",
    "    top_indexes = np.setdiff1d(top_50_indexes,[idx_1,idx_2,idx_3])\n",
    "    for i in top_indexes[:top_n]:\n",
    "        recommended_movies.append(list(movies['title'])[i])\n",
    "    return recommended_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAmiLj_RWPyS",
    "outputId": "65d145c8-3717-477a-9aa3-2429b97643d0"
   },
   "outputs": [],
   "source": [
    "movies = movies_df.dropna()\n",
    "movie_list = ['Grumpier Old Men (1995)','Ace Ventura: When Nature Calls (1995)','Father of the Bride Part II (1995)']\n",
    "content_model(movie_list,top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teKzAXIF2FyS"
   },
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## 6. Performance Evaluation\n",
    "\n",
    "We built and tested six different collaborative filtering models and compared their performance using a statistical measure known as the root mean squared error (**RMSE**), which determines the average squared difference between the estimated values and the actual value. A low RMSE value indicates high model accuracy.\n",
    "\n",
    "### Root Mean Squared Error (RMSE):\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big(\\frac{d_i -f_i}{\\sigma_i}\\Big)^2}}$$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "r51HzbnTWPyU",
    "outputId": "a23ea3f3-58c1-4da2-cf8b-225d853fc9b7"
   },
   "outputs": [],
   "source": [
    "# Compare RMSE values between models\n",
    "fig,axis = plt.subplots(figsize=(8, 5))\n",
    "rmse_x = ['SVD','NormalPredictor','BaselineOnly','NMF','SlopeOne','CoClustering']\n",
    "rmse_y = [svd_rmse,np_rmse,blo_rmse,nmf_rmse,slo_rmse,cc_rmse]\n",
    "ax = sns.barplot(x=rmse_x, y=rmse_y,palette='brg',edgecolor='black')\n",
    "plt.title('RMSE Value Per Collaborative-based Filtering Model',fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('RMSE')\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),2), fontsize=12, ha=\"center\", va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAq5zgQ1WPyV"
   },
   "source": [
    "### Cross Validation\n",
    "\n",
    "Cross validation is a technique used to test the accuracy of a model's prediction on unseen data (validation sets). This is important because it can assist in picking up issues such as over/underfitting and selection bias. We used the K-fold technique to perform cross validation on our two best perfoming models, i.e. **SVD** and **BaselineOnly**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZBbROI4WPyV"
   },
   "source": [
    "**SVD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5QtmfZgWPyW",
    "outputId": "69a35563-ea4a-4740-a545-9cb881d48d1f"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 40, n_factors = 200, init_std_dev = 0.05, random_state=42)\n",
    "# Run 5-fold cross-validation and print results\n",
    "a = cross_validate(svd_test, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNKRRWEEWPyY"
   },
   "source": [
    "**BaselineOnly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQM7wuR1WPyY",
    "outputId": "07a96880-c61d-416d-d75d-423ef39a359e"
   },
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd','n_epochs': 40}\n",
    "blo_test = BaselineOnly(bsl_options=bsl_options)\n",
    "# Run 5-fold cross-validation and print results\n",
    "b = cross_validate(blo_test, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BiBBxvR2FyY"
   },
   "source": [
    "<a id=\"analysis\"></a>\n",
    "## 7. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xphTRd1Xlww0"
   },
   "outputs": [],
   "source": [
    "# Load the 100k dataset\n",
    "train_subset = train_df[:100000]\n",
    "reader = Reader(rating_scale=(train_subset['rating'].min(), train_subset['rating'].max()))\n",
    "data = Dataset.load_from_df(train_subset[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDeeMPW-WPya"
   },
   "source": [
    "### Hyperparameter Tuning \n",
    "\n",
    "Hyperparameter tuning is the process by which a set of ideal hyperparameters are chosen for a model. A hyperparameter is a parameter for which the value is set manually and tuned to control the algorithm's learning process. We tested multiple parameters for our best performing model (i.e. **SVD**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cemEEPIIWPya",
    "outputId": "ef69cb51-7e18-4ec8-a038-2033ff2a8567"
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_epochs':[40], #[30,40,50],\n",
    "              'n_factors':[400], #[100,200,300,400],\n",
    "              'init_std_dev':[0.005], #[0.001,0.005,0.05,0.1],\n",
    "              'random_state':[42]} \n",
    "grid_SVD = GridSearchCV(SVD, cv=5, measures=['rmse'], param_grid=param_grid, n_jobs=-1)\n",
    "grid_SVD.fit(data)\n",
    "print('***Best score:***')\n",
    "print(grid_SVD.best_score['rmse'])\n",
    "print('***Best parameters:***')\n",
    "print(grid_SVD.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrhCgujuWPyd"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5HJ1If7gWPyd",
    "outputId": "2d3f7c1e-80f9-4875-ac5d-4166cb5f40bc"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 40, n_factors = 400, init_std_dev = 0.005, random_state=42)\n",
    "svd_test.fit(trainset)\n",
    "predictions = svd_test.test(testset)\n",
    "# Calculate RMSE\n",
    "svd_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "sNar9_exWPye",
    "outputId": "94eebe26-4df4-45c9-9f11-e4569f27cce7"
   },
   "outputs": [],
   "source": [
    "# Predicted Target Values vs. Actual Target Values\n",
    "new_df = pd.DataFrame(columns=['uid', 'iid', 'rating'])\n",
    "i = 0\n",
    "for (uid, iid, rating) in testset:\n",
    "    new_df.loc[i] = [uid, iid, rating]\n",
    "    i = i+1\n",
    "true = new_df['rating']\n",
    "pred = []\n",
    "for i in predictions:\n",
    "    pred.append(i.est)\n",
    "fig,axis = plt.subplots(figsize=(8, 5))\n",
    "sns.boxplot(x=true, y=pred, palette=\"brg\")\n",
    "plt.title(\"Predicted Target Values vs. Actual Target Values\", fontsize=14)\n",
    "plt.xlabel(\"Actual Target Values\")\n",
    "plt.ylabel(\"Predicted Target Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVzGHxoQL9es"
   },
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 8. Conclusion\n",
    "The singular value decomposition (SVD) algorithm is a baseline approach to recommender systems, as it has a broad range of applications including dimensionality reduction, solving linear inverse problems, and data fitting. The SVD algorithm generally performs better on large datasets compared to some other models as it decomposes a matrix into constituent arrays of feature vectors corresponding to each row and each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4eHDTwoL9et"
   },
   "source": [
    "<a id=\"save\"></a>\n",
    "## 9. Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjAQTPZmL9et",
    "outputId": "241eb386-2fa0-4a69-de13-45cb02715c2f"
   },
   "outputs": [],
   "source": [
    "# Train model on whole dataset\n",
    "reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))\n",
    "data = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "svd = SVD(n_epochs = 40, n_factors = 400, init_std_dev = 0.005, random_state=42, verbose=True)\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Create Kaggle submission file\n",
    "predictions = []\n",
    "for i, row in test_df.iterrows():\n",
    "    x = (svd.predict(row.userId, row.movieId))\n",
    "    pred = x[3]\n",
    "    predictions.append(pred)\n",
    "test_df['Id'] = test_df['userId'].map(str) +'_'+ test_df['movieId'].map(str)\n",
    "results = pd.DataFrame({\"Id\":test_df['Id'],\"rating\": predictions})\n",
    "results.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRBo98nqnGIT"
   },
   "outputs": [],
   "source": [
    "experiment.log_dataset_hash(trainset)\n",
    "\n",
    "params = {'n_epochs':40, #[30,40,50],\n",
    "          'n_factors':400, #[100,200,300],\n",
    "          'init_std_dev':0.005, #[0.005,0.05,0.1],\n",
    "          'random_state':[42]} \n",
    "metrics = {\"RMSE\": np.sqrt(mean_squared_error(true, pred))}\n",
    "\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoo2Ek6G2Fy6"
   },
   "source": [
    "[Back to top ↑](#top)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "trialmovierecomendernotebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab9da52f69d7e4a4d34379645e58363ce95c1213c4cabfd814a48a12dd5b6956"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
